{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33658531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝을 이용한 자연어처리\n",
    "# 1. 데이터 준비\n",
    "# 2. 텍스트를 표준화\n",
    "# 3. 텍스트 분할(토큰화)\n",
    "# 4. 어휘 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0671d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45816d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449e8f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i write, rewrite, and still rewrite again!!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = test_sentence.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402d2f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i write rewrite and still rewrite again'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2edff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 표준화 함수\n",
    "def standardize(text):\n",
    "    text = text.lower()\n",
    "    return \"\".join([char for char in text if char not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77067ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "def tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e781d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary 화\n",
    "vocabulary = {\"\":0, \"[UNK]\":1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e3cdd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75133244",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in dataset:\n",
    "    text = standardize(text)\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4556cf23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '[UNK]': 1,\n",
       " 'i': 2,\n",
       " 'write': 3,\n",
       " 'erase': 4,\n",
       " 'rewrite': 5,\n",
       " 'again': 6,\n",
       " 'and': 7,\n",
       " 'then': 8,\n",
       " 'a': 9,\n",
       " 'poppy': 10,\n",
       " 'blooms': 11}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict((k,v) for k, v in vocabulary.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6127909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '[UNK]': 1,\n",
       " 'i': 2,\n",
       " 'write': 3,\n",
       " 'erase': 4,\n",
       " 'rewrite': 5,\n",
       " 'again': 6,\n",
       " 'and': 7,\n",
       " 'then': 8,\n",
       " 'a': 9,\n",
       " 'poppy': 10,\n",
       " 'blooms': 11}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96b73178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "    def tokenize(self, text):\n",
    "        return text.split()\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\":0, '[UNK]' : 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v,k) for k, v in self.vocabulary.items()\n",
    "        )\n",
    "    def encode(self,text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token,1) for token in tokens]\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i,'[UNK]') for i in int_sequence\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73d2af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    'I write, erase, rewrite',\n",
    "    'Erase again, and then',\n",
    "    'A poppy blooms'\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d59ef6f-b584-4595-8ad7-2603aee1755a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '[UNK]': 1,\n",
       " 'i': 2,\n",
       " 'write': 3,\n",
       " 'erase': 4,\n",
       " 'rewrite': 5,\n",
       " 'again': 6,\n",
       " 'and': 7,\n",
       " 'then': 8,\n",
       " 'a': 9,\n",
       " 'poppy': 10,\n",
       " 'blooms': 11}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 집합에 없는 단어일 경우 UNK로 대체\n",
    "vectorizer.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56b1d0e5-fca2-454d-bdf1-8b31bb2a8d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 7, 1, 5, 6]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"I write, erase, rewrite and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "encoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46525ba3-0156-4247-a2f5-1aa1c802c646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '',\n",
       " 1: '[UNK]',\n",
       " 2: 'i',\n",
       " 3: 'write',\n",
       " 4: 'erase',\n",
       " 5: 'rewrite',\n",
       " 6: 'again',\n",
       " 7: 'and',\n",
       " 8: 'then',\n",
       " 9: 'a',\n",
       " 10: 'poppy',\n",
       " 11: 'blooms'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81306888-8886-469c-b631-78466e9f221c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i write erase rewrite and [UNK] rewrite again'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence = vectorizer.decode(encoded_sentence)\n",
    "decode_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb4354f0-2611-4445-a7d2-3e11a1207f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(output_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5fe8962-93dc-4d31-a7f9-b866a65232e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string =  tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\",\"\"\n",
    "    )\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode='int',\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c41d89cf-731c-4e59-9cfb-f990dca4c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    'I write, erase, rewrite',\n",
    "    'Erase again, and then',\n",
    "    'A poppy blooms'\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98adf94b-db7e-45ec-b445-f1a8501fb004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어휘 사전 출력\n",
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "873a7e1a-4f53-4d0b-a533-48bda147b4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=int64, numpy=array([ 7,  3,  5,  9,  1,  5, 10], dtype=int64)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encode_sentence = text_vectorization(test_sentence)\n",
    "encode_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50f56593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i write rewrite and [UNK] rewrite again'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decode_sentence = \" \".join(inverse_vocab[int(i)] for i in encode_sentence)\n",
    "decode_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "252c9031-b23c-4e98-921c-3983229b2b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 그룹을 표현하는 두 가지 방법: 집합과 시퀀스\n",
    "# IMDB 영화 리뷰 데이터 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed694209-f69b-4a0b-8011-96adc028f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리눅스나 코렙에서 사용가능\n",
    "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !tar -xf aclImdb_v1.tar.gz\n",
    "# !rm -r aclImdb/train/unsup\n",
    "# !cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8e53892-53f3-4f89-b72b-744de0d2f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req    \n",
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "filename = 'aclImdb_v1.tar.gz'\n",
    "with req.urlopen(url) as f:\n",
    "    with open(filename,'wb') as of:\n",
    "        of.write(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21889ed9-e311-48b8-8dd7-9e484c066302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open(filename,'r:gz') as tr:\n",
    "    tr.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "262ac429-905d-4e60-85dd-69ab77a5dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4fc9b05b-aee8-4006-8d4a-87e9ae7239be",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = pathlib.Path('aclImdb')\n",
    "val_dir = base_dir / 'val'\n",
    "train_dir = base_dir / 'train'\n",
    "for category in ('neg', 'pos'):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2*len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb1d3785-592c-4bc7-90c0-07032e43414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9e02618d-3c58-40cf-b597-477aa7fd3dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/val', batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test', batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4557dab7-6ad8-46ff-b510-6bd2e3becddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape : (32,)\n",
      "inputs.dtype : <dtype: 'string'>\n",
      "targets.shape : (32,)\n",
      "targets.dtype : <dtype: 'int32'>\n",
      "inputs[0] : b'Despite being released on DVD by Blue Underground some five years ago, I have never come across this Italian \"sword and sorcery\" item on late-night Italian TV and, now that I have seen it for myself, I know exactly why. Not because of its director\\'s typical predilection for extreme gore (of which there is some examples to be sure) or the fact that the handful of women in it parade topless all the time (it is set in the Dark Ages after all)\\xc2\\x85it is, quite simply, very poor stuff indeed. In fact, I would go so far as to say that it may very well be the worst of its kind that I have yet seen and, believe me, I have seen plenty (especially in the last few years i.e. following my excursion to the 2004 Venice Film Festival)! Reading about how the film\\'s failure at the time of initial release is believed to have led to its director\\'s subsequent (and regrettable) career nosedive into mindless low-budget gore, I can see their point: I may prefer Fulci\\'s earlier \"giallo\" period (1968-77) to his more popular stuff horror (1979-82) myself but, even on the latter, his commitment was arguably unquestionable. On the other hand, CONQUEST seems not to have inspired Fulci in the least \\xc2\\x96 seeing how he decided to drape the proceedings with an annoyingly perpetual mist, sprinkle it with incongruent characters (cannibals vs. werewolves, anyone?), irrelevant gore (we are treated to a gratuitous, nasty cannibal dinner just before witnessing the flesh-eating revelers having their brains literally beaten out by their hairy antagonists!) and even some highly unappetizing intimacy between the masked, brain-slurping villainess (don\\'t ask) and her slimy reptilian pet!! For what it is worth, we have two heroes for the price of one here: a young magic bow-carrying boy on some manhood-affirming odyssey (Andrea Occhipinti) and his rambling muscle-bound companion (Jorge Rivero i.e. Frenchy from Howard Hawks\\' RIO LOBO [1970]!) who, despite being called Mace (short for Maciste, perhaps?), seems to be there simply to drop in on his cavewoman from time to time and get his younger prot\\xc3\\xa9g\\xc3\\xa9 out of trouble (particularly during an exceedingly unpleasant attack of the \\'boils\\'). Unfortunately, even the usual saving grace of such lowbrow material comes up short here as ex-Goblin Claudio Simonetti\\'s electronic score seems awfully inappropriate at times. Fulci even contrives to give the film a laughably hurried coda with the surviving beefy hero going aimlessly out into the wilderness (after defeating one and all with the aid of the all-important magic bow\\xc2\\x85so much for his own supposed physical strength!) onto his next \\xc2\\x96 and thankfully unfilmed \\xc2\\x96 adventure!'\n",
      "targets[0] : 0\n"
     ]
    }
   ],
   "source": [
    "for inputs,targets in train_ds:\n",
    "    print(f\"inputs.shape : {inputs.shape}\")\n",
    "    print(f\"inputs.dtype : {inputs.dtype}\")\n",
    "    print(f\"targets.shape : {targets.shape}\")\n",
    "    print(f\"targets.dtype : {targets.dtype}\")\n",
    "    print(f\"inputs[0] : {inputs[0]}\")\n",
    "    print(f\"targets[0] : {targets[0]}\")\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "edb3577e-ecd5-4848-872f-9cfe80d6b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어를 집합으로 처리  : BoW 방식\n",
    "# TextVectorization층으로 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8de718c7-f16b-42a2-98a2-8936f8dd201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization =  TextVectorization(\n",
    "    max_tokens=20000 ,\n",
    "    output_mode='multi_hot'\n",
    ")\n",
    "text_only_train_ds =  train_ds.map(lambda x, y : x)\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fef3b67a-102c-4e5e-92d7-a805d154c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_1gram_train_ds =  train_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "binary_1gram_val_ds =  val_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "binary_1gram_test_ds =  test_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b975400d-ef0d-448c-8f20-1b658ae6b67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape : (32, 20000)\n",
      "inputs.dtype : <dtype: 'float32'>\n",
      "targets.shape : (32,)\n",
      "targets.dtype : <dtype: 'int32'>\n",
      "inputs[0] : [1. 1. 1. ... 0. 0. 0.]\n",
      "targets[0] : 1\n"
     ]
    }
   ],
   "source": [
    "for inputs,targets in binary_1gram_train_ds:\n",
    "    print(f\"inputs.shape : {inputs.shape}\")\n",
    "    print(f\"inputs.dtype : {inputs.dtype}\")\n",
    "    print(f\"targets.shape : {targets.shape}\")\n",
    "    print(f\"targets.dtype : {targets.dtype}\")\n",
    "    print(f\"inputs[0] : {inputs[0]}\")\n",
    "    print(f\"targets[0] : {targets[0]}\")\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d0b58ae5-5199-4fb4-9cc5-c94dd16e7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "adc442f8-1fa6-4c63-b614-4db89aa6df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "def get_model(max_tokens = 20000, hidden_dim = 16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation='sigmoid')(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1,activation = 'sigmoid')(x)\n",
    "    model = keras.Model(inputs,outputs)\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "52be2d34-d2dc-4420-97f3-36f592b8d761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 이진 유니그램 모델 훈련하고 테스트\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fda91ec1-a7bb-4cb7-b050-9730cd29eafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.5101 - accuracy: 0.7491 - val_loss: 0.3609 - val_accuracy: 0.8704\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.3319 - accuracy: 0.8647 - val_loss: 0.2973 - val_accuracy: 0.8804\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2790 - accuracy: 0.8863 - val_loss: 0.2839 - val_accuracy: 0.8830\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2491 - accuracy: 0.9029 - val_loss: 0.2811 - val_accuracy: 0.8876\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2287 - accuracy: 0.9123 - val_loss: 0.2846 - val_accuracy: 0.8866\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2124 - accuracy: 0.9209 - val_loss: 0.2878 - val_accuracy: 0.8898\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2086 - accuracy: 0.9241 - val_loss: 0.2923 - val_accuracy: 0.8892\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2000 - accuracy: 0.9291 - val_loss: 0.2976 - val_accuracy: 0.8914\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1924 - accuracy: 0.9304 - val_loss: 0.3030 - val_accuracy: 0.8914\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1862 - accuracy: 0.9354 - val_loss: 0.3074 - val_accuracy: 0.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x262707b43d0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('binary_1gram.keras', save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e4823a7b-3ceb-45f2-82b8-cb907ff89494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 인코딩을 사용한 바이그램\n",
    "#바이그램을 반환하는 TextVectorization 층 만들기\n",
    "# 바이그램 : 문자나 음절 또는 단어인 코든 문자열에서 인접한 두 요소의 시권스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c0728d22-8afd-47d4-80ea-b23aba60be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens = 20000,\n",
    "    output_mode = 'multi_hot'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "63056e99-fc85-4208-a871-3ca88b1b6170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 0.5008 - accuracy: 0.7535 - val_loss: 0.3520 - val_accuracy: 0.8704\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.3313 - accuracy: 0.8625 - val_loss: 0.2959 - val_accuracy: 0.8794\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2793 - accuracy: 0.8920 - val_loss: 0.2857 - val_accuracy: 0.8838\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2459 - accuracy: 0.9049 - val_loss: 0.2831 - val_accuracy: 0.8878\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2257 - accuracy: 0.9153 - val_loss: 0.2848 - val_accuracy: 0.8924\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2143 - accuracy: 0.9196 - val_loss: 0.2902 - val_accuracy: 0.8930\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2036 - accuracy: 0.9252 - val_loss: 0.2952 - val_accuracy: 0.8938\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1951 - accuracy: 0.9288 - val_loss: 0.2994 - val_accuracy: 0.8916\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1879 - accuracy: 0.9326 - val_loss: 0.3052 - val_accuracy: 0.8920\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1819 - accuracy: 0.9347 - val_loss: 0.3112 - val_accuracy: 0.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26200245550>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_only_train_ds =  train_ds.map(lambda x, y : x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds =  train_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "binary_2gram_val_ds =  val_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "binary_2gram_test_ds =  test_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "model = get_model()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('binary_2gram.keras', save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "645bbf71-265a-4963-b49b-638a64858095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF인코딩을 사용한 바이어그램\n",
    "# 토큰 카운트를 반환하는 TextVectorization 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9eac641e-85f1-4475-84c8-7b469eda2f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens = 20000,\n",
    "    output_mode = 'count'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c62661c8-809e-49cd-b3a3-344b6706bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 가중치가 적용된 출력을 반환하는 TextBectorization 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d2ff6836-e7c2-49f9-90c5-70a506cc683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization =  TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode = 'tf_idf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "23385463-3c95-466a-ac4f-fc81709d73d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d67fcd21-82bc-4e4d-9cdc-e12e2109057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 2.8.xx 이상에서는 gup에서 오류 - 2.9에서는 해결\n",
    "# with tf.device('cpu'):\n",
    "#     text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7a43a322-a424-4fe3-93a1-ddd14a9b1fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.3993 - accuracy: 0.8243 - val_loss: 0.2696 - val_accuracy: 0.9020\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2394 - accuracy: 0.9133 - val_loss: 0.2430 - val_accuracy: 0.9060\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1951 - accuracy: 0.9327 - val_loss: 0.2575 - val_accuracy: 0.9042\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1630 - accuracy: 0.9453 - val_loss: 0.2591 - val_accuracy: 0.9066\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1452 - accuracy: 0.9527 - val_loss: 0.2737 - val_accuracy: 0.9024\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1320 - accuracy: 0.9591 - val_loss: 0.2926 - val_accuracy: 0.9042\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1182 - accuracy: 0.9632 - val_loss: 0.2928 - val_accuracy: 0.9044\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1096 - accuracy: 0.9679 - val_loss: 0.3150 - val_accuracy: 0.9016\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1031 - accuracy: 0.9693 - val_loss: 0.3182 - val_accuracy: 0.9000\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.0939 - accuracy: 0.9730 - val_loss: 0.3401 - val_accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x262007f5460>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_only_train_ds =  train_ds.map(lambda x, y : x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "tfidf_2gram_train_ds =  train_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds =  val_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds =  test_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "model = get_model()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('tfidf_2gram.keras', save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5258b16e-3d92-42e2-ac22-6756e0b64b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype='string')\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "outputs = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4ccc0368-7409-4652-93fe-0dd7e1b7582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_data = tf.convert_to_tensor([\n",
    "    ['That was an excellent movie, i hate it.']    \n",
    "])\n",
    "raw_text_data2 = tf.convert_to_tensor([\n",
    "['The movie was boring in the first half, but it got more and more interesting. Disadvantages of having a lot of CG']\n",
    "])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9575a89b-abd8-4f35-a1cd-8a2bfd255c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정적인 리뷰일  확율 : 98.97091674804688\n",
      "긍정적인 리뷰일  확율 : 13.248270988464355\n"
     ]
    }
   ],
   "source": [
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"긍정적인 리뷰일  확율 : {float(predictions[0]*100)}\")\n",
    "\n",
    "predictions = inference_model(raw_text_data2)\n",
    "print(f\"긍정적인 리뷰일  확율 : {float(predictions[0]*100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572b90c-e806-4c29-abfe-df7cf36f3cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
