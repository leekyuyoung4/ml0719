{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33658531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝을 이용한 자연어처리\n",
    "# 1. 데이터 준비\n",
    "# 2. 텍스트를 표준화\n",
    "# 3. 텍스트 분할(토큰화)\n",
    "# 4. 어휘 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0671d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45816d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449e8f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i write, rewrite, and still rewrite again!!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = test_sentence.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402d2f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i write rewrite and still rewrite again'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2edff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 표준화 함수\n",
    "def standardize(text):\n",
    "    text = text.lower()\n",
    "    return \"\".join([char for char in text if char not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77067ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "def tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e781d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary 화\n",
    "vocabulary = {\"\":0, \"[UNK]\":1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e3cdd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75133244",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in dataset:\n",
    "    text = standardize(text)\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4556cf23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '[UNK]': 1,\n",
       " 'i': 2,\n",
       " 'write': 3,\n",
       " 'erase': 4,\n",
       " 'rewrite': 5,\n",
       " 'again': 6,\n",
       " 'and': 7,\n",
       " 'then': 8,\n",
       " 'a': 9,\n",
       " 'poppy': 10,\n",
       " 'blooms': 11}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict((k,v) for k, v in vocabulary.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6127909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '[UNK]': 1,\n",
       " 'i': 2,\n",
       " 'write': 3,\n",
       " 'erase': 4,\n",
       " 'rewrite': 5,\n",
       " 'again': 6,\n",
       " 'and': 7,\n",
       " 'then': 8,\n",
       " 'a': 9,\n",
       " 'poppy': 10,\n",
       " 'blooms': 11}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96b73178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "    def tokenize(self, text):\n",
    "        return text.split()\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\":0, '[UNK]' : 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v,k) for k, v in self.vocabulary.items()\n",
    "        )\n",
    "    def encode(self,text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token,1) for token in tokens]\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i,'[UNK]') for i in int_sequence\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73d2af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    'I write, erase, rewrite',\n",
    "    'Erase again, and then',\n",
    "    'A poppy blooms'\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d59ef6f-b584-4595-8ad7-2603aee1755a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '[UNK]': 1,\n",
       " 'i': 2,\n",
       " 'write': 3,\n",
       " 'erase': 4,\n",
       " 'rewrite': 5,\n",
       " 'again': 6,\n",
       " 'and': 7,\n",
       " 'then': 8,\n",
       " 'a': 9,\n",
       " 'poppy': 10,\n",
       " 'blooms': 11}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 집합에 없는 단어일 경우 UNK로 대체\n",
    "vectorizer.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56b1d0e5-fca2-454d-bdf1-8b31bb2a8d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 7, 1, 5, 6]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"I write, erase, rewrite and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "encoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46525ba3-0156-4247-a2f5-1aa1c802c646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '',\n",
       " 1: '[UNK]',\n",
       " 2: 'i',\n",
       " 3: 'write',\n",
       " 4: 'erase',\n",
       " 5: 'rewrite',\n",
       " 6: 'again',\n",
       " 7: 'and',\n",
       " 8: 'then',\n",
       " 9: 'a',\n",
       " 10: 'poppy',\n",
       " 11: 'blooms'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81306888-8886-469c-b631-78466e9f221c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i write erase rewrite and [UNK] rewrite again'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence = vectorizer.decode(encoded_sentence)\n",
    "decode_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb4354f0-2611-4445-a7d2-3e11a1207f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(output_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5fe8962-93dc-4d31-a7f9-b866a65232e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string =  tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\",\"\"\n",
    "    )\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode='int',\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c41d89cf-731c-4e59-9cfb-f990dca4c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    'I write, erase, rewrite',\n",
    "    'Erase again, and then',\n",
    "    'A poppy blooms'\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98adf94b-db7e-45ec-b445-f1a8501fb004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어휘 사전 출력\n",
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "873a7e1a-4f53-4d0b-a533-48bda147b4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=int64, numpy=array([ 7,  3,  5,  9,  1,  5, 10], dtype=int64)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encode_sentence = text_vectorization(test_sentence)\n",
    "encode_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50f56593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i write rewrite and [UNK] rewrite again'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decode_sentence = \" \".join(inverse_vocab[int(i)] for i in encode_sentence)\n",
    "decode_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "252c9031-b23c-4e98-921c-3983229b2b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 그룹을 표현하는 두 가지 방법: 집합과 시퀀스\n",
    "# IMDB 영화 리뷰 데이터 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed694209-f69b-4a0b-8011-96adc028f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리눅스나 코렙에서 사용가능\n",
    "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !tar -xf aclImdb_v1.tar.gz\n",
    "# !rm -r aclImdb/train/unsup\n",
    "# !cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8e53892-53f3-4f89-b72b-744de0d2f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req    \n",
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "filename = 'aclImdb_v1.tar.gz'\n",
    "with req.urlopen(url) as f:\n",
    "    with open(filename,'wb') as of:\n",
    "        of.write(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21889ed9-e311-48b8-8dd7-9e484c066302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open(filename,'r:gz') as tr:\n",
    "    tr.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "262ac429-905d-4e60-85dd-69ab77a5dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4fc9b05b-aee8-4006-8d4a-87e9ae7239be",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = pathlib.Path('aclImdb')\n",
    "val_dir = base_dir / 'val'\n",
    "train_dir = base_dir / 'train'\n",
    "for category in ('neg', 'pos'):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2*len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb1d3785-592c-4bc7-90c0-07032e43414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e02618d-3c58-40cf-b597-477aa7fd3dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70000 files belonging to 3 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/val', batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test', batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4557dab7-6ad8-46ff-b510-6bd2e3becddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape : (32,)\n",
      "inputs.dtype : <dtype: 'string'>\n",
      "targets.shape : (32,)\n",
      "targets.dtype : <dtype: 'int32'>\n",
      "inputs[0] : b\"This David Hamilton movie is something of a disaster. It has a half-decent story (coming-of-age/love triangle) and a mediocre script. All of Hamilton's usual trademarks are here - stunningly beautiful & innocent young girls, soft-focus photography, lots of nudity, gentle lesbianism, and a romantic soundtrack. There is/was a good movie to be made with the given material & cast, but this is not it. The editing is appalling, and at times the scene shifts seem somewhat disjointed.<br /><br />It's worth remembering that Hamilton was an iconic photographer of his time and experimented with taking his art into the movie field, in many ways pioneering the penchant for photographers in the adult industry to also dabble in video work.<br /><br />However, on the plus side, there are very good performances from Dawn Dunlap (as Laura) and James Mitchell (as Paul), and the soundtrack is quite nice.<br /><br />To summarise, I'd say that unless you're a fan of Hamilton's other work, e.g. Bilitis, Summer in St Tropez, then this is best forgotten.<br /><br />I rate it 4/10.\"\n",
      "targets[0] : 2\n"
     ]
    }
   ],
   "source": [
    "for inputs,targets in train_ds:\n",
    "    print(f\"inputs.shape : {inputs.shape}\")\n",
    "    print(f\"inputs.dtype : {inputs.dtype}\")\n",
    "    print(f\"targets.shape : {targets.shape}\")\n",
    "    print(f\"targets.dtype : {targets.dtype}\")\n",
    "    print(f\"inputs[0] : {inputs[0]}\")\n",
    "    print(f\"targets[0] : {targets[0]}\")\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "edb3577e-ecd5-4848-872f-9cfe80d6b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어를 집합으로 처리  : BoW 방식\n",
    "# TextVectorization층으로 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8de718c7-f16b-42a2-98a2-8936f8dd201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectmaprization =  TextVectorization(\n",
    "    max_tokens=70000 ,\n",
    "    output_mode='multi_hot'\n",
    ")\n",
    "text_only_train_ds =  train_ds.map(lambda x, y : x)\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fef3b67a-102c-4e5e-92d7-a805d154c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_1gram_train_ds =  train_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "binary_1gram_val_ds =  val_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "binary_1gram_test_ds =  test_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b975400d-ef0d-448c-8f20-1b658ae6b67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape : (32, 70000)\n",
      "inputs.dtype : <dtype: 'float32'>\n",
      "targets.shape : (32,)\n",
      "targets.dtype : <dtype: 'int32'>\n",
      "inputs[0] : [0. 1. 1. ... 0. 0. 0.]\n",
      "targets[0] : 0\n"
     ]
    }
   ],
   "source": [
    "for inputs,targets in binary_1gram_test_ds:\n",
    "    print(f\"inputs.shape : {inputs.shape}\")\n",
    "    print(f\"inputs.dtype : {inputs.dtype}\")\n",
    "    print(f\"targets.shape : {targets.shape}\")\n",
    "    print(f\"targets.dtype : {targets.dtype}\")\n",
    "    print(f\"inputs[0] : {inputs[0]}\")\n",
    "    print(f\"targets[0] : {targets[0]}\")\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0b58ae5-5199-4fb4-9cc5-c94dd16e7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "adc442f8-1fa6-4c63-b614-4db89aa6df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "def get_model(max_tokens = 70000, hidden_dim = 16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation='sigmoid')(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1,activation = 'sigmoid')(x)\n",
    "    model = keras.Model(inputs,outputs)\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "52be2d34-d2dc-4420-97f3-36f592b8d761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 70000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                1120016   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,120,033\n",
      "Trainable params: 1,120,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 이진 유니그램 모델 훈련하고 테스트\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda91ec1-a7bb-4cb7-b050-9730cd29eafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 936/2188 [===========>..................] - ETA: 3:46 - loss: -5.0535 - accuracy: 0.1421"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('binary_1gram.keras', save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4823a7b-3ceb-45f2-82b8-cb907ff89494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
