{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iQ8vXYhSpvoP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "DEVICE    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13UjUF4EvFOv",
        "outputId": "8365bcee-0055-4f46-c8ee-ecc0dd8f7ed9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "ocTJO--uvp31"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root = '../data/CIFAR_10',train=True, download = True, transform = transforms.ToTensor())\n",
        "test_dataset = datasets.CIFAR10(root = '../data/CIFAR_10',train=False, download = True, transform = transforms.ToTensor())\n",
        "train_loader = torch.utils.data.DataLoader(dataset= train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset= test_dataset,batch_size=BATCH_SIZE,shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ImjHGrfvwUV",
        "outputId": "d7fa9242-328d-4f14-f90d-9188e0c27437"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data,label in  train_dataset:\n",
        "  print(data.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NFa6EA8xA62",
        "outputId": "b70df8fc-c756-4df4-fde5-94286577ead2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 설계\n",
        "class Net(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "     super().__init__()\n",
        "     self.fc1 = nn.Linear(3*32*32,512)\n",
        "     self.fc2 = nn.Linear(512,256)\n",
        "     self.fc3 = nn.Linear(256,10)\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1,3*32*32)\n",
        "    x = self.fc1(x)\n",
        "    x = torch.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = torch.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "    return x  "
      ],
      "metadata": {
        "id": "_oeAMC85wdtN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net().to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "j9XN0WCSxuF1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, train_loader, optimizer):\n",
        "  model.train() # 학습\n",
        "  # 모델 성능 확인\n",
        "  for batch_idx, (data,label) in tqdm(enumerate(train_loader)):\n",
        "    data = data.to(DEVICE)\n",
        "    label = label.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = criterion(output, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 출력\n",
        "    # output_str =  f\"train epoch : {batch_idx} {len(data)}/{len(train_loader.dataset):.0f} train_loss : {loss.item():.6f}\"\n",
        "    # print(output_str)"
      ],
      "metadata": {
        "id": "T6FqJERCyd4s"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, label in test_loader:\n",
        "      data = data.to(DEVICE)\n",
        "      label = label.to(DEVICE)\n",
        "      output = model(data)\n",
        "      test_loss += criterion(output,label).item()\n",
        "      prediction = output.max(1, keepdim =True)[1]\n",
        "      correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_accuracy = 100.*correct / len(test_loader.dataset)\n",
        "  return test_loss, test_accuracy"
      ],
      "metadata": {
        "id": "1__IkYTozWFF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습"
      ],
      "metadata": {
        "id": "ZHP7N2szzU1-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, EPOCHS+1):\n",
        "  train(model,train_loader,optimizer)\n",
        "  test_loss, test_accuracy =  evaluate(model, test_loader)\n",
        "  print(f\"EPOCH:{epoch} , test loss:{test_loss:.4f}, test Accuracy:{test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6-gbJ_i2xjl",
        "outputId": "05a982ce-3232-4791-c66a-149985a75861"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1563it [00:08, 191.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:1 , test loss:0.0505, test Accuracy:42.43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1563it [00:08, 190.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:2 , test loss:0.0482, test Accuracy:45.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1563it [00:08, 190.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:3 , test loss:0.0480, test Accuracy:45.72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1563it [00:08, 189.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:4 , test loss:0.0479, test Accuracy:45.42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1563it [00:08, 189.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:5 , test loss:0.0458, test Accuracy:47.76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1563it [00:08, 190.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:6 , test loss:0.0465, test Accuracy:46.78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1563it [00:08, 188.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:7 , test loss:0.0458, test Accuracy:48.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1563it [00:08, 192.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:8 , test loss:0.0458, test Accuracy:48.29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1563it [00:08, 191.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:9 , test loss:0.0475, test Accuracy:46.43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1563it [00:08, 187.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:10 , test loss:0.0448, test Accuracy:49.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 데이터와 모델이 예측한 이미지 비교해보기\n",
        "sampleDatas = []\n",
        "for idx,datas in enumerate(test_dataset):\n",
        "  if idx > 10: break\n",
        "  sampleDatas.append(datas)  "
      ],
      "metadata": {
        "id": "26SpFztN3hum"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V-w7Nzyo6sTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = np.transpose(image,axes=(1,2,0))\n",
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWECPoVA47CE",
        "outputId": "cabcb600-2887-468a-db83-6e664435a14f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 32, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image), label\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "_39ia71e5EB0",
        "outputId": "db758110-2b1b-4b5f-aa14-e4a82f2c5447"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<matplotlib.image.AxesImage at 0x7f83c120bb50>, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(image.view)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbWos81k5O4n",
        "outputId": "9fd36df0-e1be-4414-89bf-6535a68f8eb5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on built-in function view:\n",
            "\n",
            "view(...) method of torch.Tensor instance\n",
            "    view(*shape) -> Tensor\n",
            "    \n",
            "    Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
            "    different :attr:`shape`.\n",
            "    \n",
            "    The returned tensor shares the same data and must have the same number\n",
            "    of elements, but may have a different size. For a tensor to be viewed, the new\n",
            "    view size must be compatible with its original size and stride, i.e., each new\n",
            "    view dimension must either be a subspace of an original dimension, or only span\n",
            "    across original dimensions :math:`d, d+1, \\dots, d+k` that satisfy the following\n",
            "    contiguity-like condition that :math:`\\forall i = d, \\dots, d+k-1`,\n",
            "    \n",
            "    .. math::\n",
            "    \n",
            "      \\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]\n",
            "    \n",
            "    Otherwise, it will not be possible to view :attr:`self` tensor as :attr:`shape`\n",
            "    without copying it (e.g., via :meth:`contiguous`). When it is unclear whether a\n",
            "    :meth:`view` can be performed, it is advisable to use :meth:`reshape`, which\n",
            "    returns a view if the shapes are compatible, and copies (equivalent to calling\n",
            "    :meth:`contiguous`) otherwise.\n",
            "    \n",
            "    Args:\n",
            "        shape (torch.Size or int...): the desired size\n",
            "    \n",
            "    Example::\n",
            "    \n",
            "        >>> x = torch.randn(4, 4)\n",
            "        >>> x.size()\n",
            "        torch.Size([4, 4])\n",
            "        >>> y = x.view(16)\n",
            "        >>> y.size()\n",
            "        torch.Size([16])\n",
            "        >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
            "        >>> z.size()\n",
            "        torch.Size([2, 8])\n",
            "    \n",
            "        >>> a = torch.randn(1, 2, 3, 4)\n",
            "        >>> a.size()\n",
            "        torch.Size([1, 2, 3, 4])\n",
            "        >>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
            "        >>> b.size()\n",
            "        torch.Size([1, 3, 2, 4])\n",
            "        >>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n",
            "        >>> c.size()\n",
            "        torch.Size([1, 3, 2, 4])\n",
            "        >>> torch.equal(b, c)\n",
            "        False\n",
            "    \n",
            "    \n",
            "    .. method:: view(dtype) -> Tensor\n",
            "       :noindex:\n",
            "    \n",
            "    Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
            "    different :attr:`dtype`.\n",
            "    \n",
            "    If the element size of :attr:`dtype` is different than that of ``self.dtype``,\n",
            "    then the size of the last dimension of the output will be scaled\n",
            "    proportionally.  For instance, if :attr:`dtype` element size is twice that of\n",
            "    ``self.dtype``, then each pair of elements in the last dimension of\n",
            "    :attr:`self` will be combined, and the size of the last dimension of the output\n",
            "    will be half that of :attr:`self`. If :attr:`dtype` element size is half that\n",
            "    of ``self.dtype``, then each element in the last dimension of :attr:`self` will\n",
            "    be split in two, and the size of the last dimension of the output will be\n",
            "    double that of :attr:`self`. For this to be possible, the following conditions\n",
            "    must be true:\n",
            "    \n",
            "        * ``self.dim()`` must be greater than 0.\n",
            "        * ``self.stride(-1)`` must be 1.\n",
            "    \n",
            "    Additionally, if the element size of :attr:`dtype` is greater than that of\n",
            "    ``self.dtype``, the following conditions must be true as well:\n",
            "    \n",
            "        * ``self.size(-1)`` must be divisible by the ratio between the element\n",
            "          sizes of the dtypes.\n",
            "        * ``self.storage_offset()`` must be divisible by the ratio between the\n",
            "          element sizes of the dtypes.\n",
            "        * The strides of all dimensions, except the last dimension, must be\n",
            "          divisible by the ratio between the element sizes of the dtypes.\n",
            "    \n",
            "    If any of the above conditions are not met, an error is thrown.\n",
            "    \n",
            "    .. warning::\n",
            "    \n",
            "        This overload is not supported by TorchScript, and using it in a Torchscript\n",
            "        program will cause undefined behavior.\n",
            "    \n",
            "    \n",
            "    Args:\n",
            "        dtype (:class:`torch.dtype`): the desired dtype\n",
            "    \n",
            "    Example::\n",
            "    \n",
            "        >>> x = torch.randn(4, 4)\n",
            "        >>> x\n",
            "        tensor([[ 0.9482, -0.0310,  1.4999, -0.5316],\n",
            "                [-0.1520,  0.7472,  0.5617, -0.8649],\n",
            "                [-2.4724, -0.0334, -0.2976, -0.8499],\n",
            "                [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
            "        >>> x.dtype\n",
            "        torch.float32\n",
            "    \n",
            "        >>> y = x.view(torch.int32)\n",
            "        >>> y\n",
            "        tensor([[ 1064483442, -1124191867,  1069546515, -1089989247],\n",
            "                [-1105482831,  1061112040,  1057999968, -1084397505],\n",
            "                [-1071760287, -1123489973, -1097310419, -1084649136],\n",
            "                [-1101533110,  1073668768, -1082790149, -1088634448]],\n",
            "            dtype=torch.int32)\n",
            "        >>> y[0, 0] = 1000000000\n",
            "        >>> x\n",
            "        tensor([[ 0.0047, -0.0310,  1.4999, -0.5316],\n",
            "                [-0.1520,  0.7472,  0.5617, -0.8649],\n",
            "                [-2.4724, -0.0334, -0.2976, -0.8499],\n",
            "                [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
            "    \n",
            "        >>> x.view(torch.cfloat)\n",
            "        tensor([[ 0.0047-0.0310j,  1.4999-0.5316j],\n",
            "                [-0.1520+0.7472j,  0.5617-0.8649j],\n",
            "                [-2.4724-0.0334j, -0.2976-0.8499j],\n",
            "                [-0.2109+1.9913j, -0.9607-0.6123j]])\n",
            "        >>> x.view(torch.cfloat).size()\n",
            "        torch.Size([4, 2])\n",
            "    \n",
            "        >>> x.view(torch.uint8)\n",
            "        tensor([[  0, 202, 154,  59, 182, 243, 253, 188, 185, 252, 191,  63, 240,  22,\n",
            "                   8, 191],\n",
            "                [227, 165,  27, 190, 128,  72,  63,  63, 146, 203,  15,  63,  22, 106,\n",
            "                  93, 191],\n",
            "                [205,  59,  30, 192, 112, 206,   8, 189,   7,  95, 152, 190,  12, 147,\n",
            "                  89, 191],\n",
            "                [ 43, 246,  87, 190, 235, 226, 254,  63, 111, 240, 117, 191, 177, 191,\n",
            "                  28, 191]], dtype=torch.uint8)\n",
            "        >>> x.view(torch.uint8).size()\n",
            "        torch.Size([4, 16])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jQcthciw5oDr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}